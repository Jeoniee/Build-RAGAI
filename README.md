# Build-RAGAI
Description

This project seeks to teach you how to build Python applications with generative AI functionality by using the LangChain and Transformers libraries.


[1ì£¼ì°¨] ê¸°ë³¸ê³¼ì œ - MNISTë¥¼ ë¶„ë¥˜ ëª¨ë¸ë¡œ í•™ìŠµí•˜ê¸°

[1ì£¼ì°¨] ì‹¬í™”ê³¼ì œ - MNIST ì˜ˆì¸¡ ëª¨ë¸ì— deep learning techniques ì ìš©í•˜ê¸°

[2ì£¼ì°¨] ê¸°ë³¸ê³¼ì œ - ì£¼ì–´ì§„ ë¬¸ì¥ì—ì„œ ë‚˜ì˜¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ êµ¬í˜„

[2ì£¼ì°¨] ì‹¬í™”ê³¼ì œ - Multi-head Attentionìœ¼ë¡œ ê°ì • ë¶„ì„ ëª¨ë¸ êµ¬í˜„í•˜ê¸°

[3ì£¼ì°¨] ê¸°ë³¸ê³¼ì œ - DistilBERTë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµí•˜ê¸°

[3ì£¼ì°¨] ì‹¬í™”ê³¼ì œ - Pre-trained ëª¨ë¸ë¡œ íš¨ìœ¨ì ì¸ NLP ëª¨ë¸ í•™ìŠµí•˜ê¸°





# ğŸ§  Last Word Prediction with Transformer

ê°„ë‹¨í•œ ë¬¸ì¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, Transformer ëª¨ë¸ì´ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•œ ì‹¤í—˜ì…ë‹ˆë‹¤.  
Self-Attentionë¶€í„° Multi-Head Attention, Residual Connection, LayerNorm, Dropoutê¹Œì§€ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ì ìš©í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°

- Embedding + Positional Encoding
- Multi-Head Self-Attention (4-head)
- Feed-Forward Network (FFN)
- Residual Connection + Layer Normalization + Dropout
- ë§ˆì§€ë§‰ `[CLS]` ìœ„ì¹˜ë¥¼ í™œìš©í•´ ë‹¨ì–´ ë¶„ë¥˜


Test Your First Notebook
If you're totally new to building AI powered applications with access to external data, specifically retrieval augmented generation, check out the RAG Basics notebook. It's the most straightforward notebook, and its concepts are built upon in every other 'RAG' notebook.

Google Colab
Click the badge below to open the RAG Basics notebook in Colab.

Open 'rag_basics.ipynb' In Colab


```text
Input â†’ Embedding + Positional Encoding
      â†’ [Transformer Layer Ã— 5]
      â†’ Linear(vocab_size) â†’ Predicted token


ì…ë ¥: "I love"
ëª¨ë¸ ì˜ˆì¸¡: "you"








